{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e17829",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4a575",
   "metadata": {},
   "source": [
    "Twitter Sentiment Analysis is the process of using Python to understand the emotions or opinions expressed in tweets automatically. By analyzing the text we can classify tweets as positive, negative or neutral. This helps businesses and researchers track public mood, brand reputation or reactions to events in real time. Python libraries like TextBlob, Tweepy and NLTK make it easy to collect tweets, process the text and perform sentiment analysis efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06169b5e",
   "metadata": {},
   "source": [
    "### How is Twitter Sentiment Analysis Useful?\n",
    "\n",
    "=> Twitter Sentiment Analysis is important because it helps people and businesses understand what the public thinks in real time.\n",
    "\n",
    "=> Millions of tweets are posted every day, sharing opinions about brands, products, events or social issues. By analyzing this huge stream of data, companies can measure customer satisfaction, spot trends early, handle negative feedback quickly and make better decisions based on how people actually feel.\n",
    "\n",
    "=> Itâ€™s also useful for researchers and governments to monitor public mood during elections, crises or big events as it turns raw tweets into valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd4375",
   "metadata": {},
   "source": [
    "## Step by Step Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7b777",
   "metadata": {},
   "source": [
    "### Step 1: Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afeb0de",
   "metadata": {},
   "source": [
    "This block installs and imports the required libraries. It uses pandas to load and handle data, TfidfVectorizer to turn text into numbers and scikit learn to train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb4f3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316df36",
   "metadata": {},
   "source": [
    "### Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e4ed6c",
   "metadata": {},
   "source": [
    "Here we loads the Sentiment140 dataset from a zipped CSV file, you can download it from Kaggle.\n",
    "\n",
    "We keep only the polarity and tweet text columns, renames them for clarity and prints the first few rows to check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4545e4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   polarity                                               text\n",
      "0         0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1         0  is upset that he can't update his Facebook by ...\n",
      "2         0  @Kenichan I dived many times for the ball. Man...\n",
      "3         0    my whole body feels itchy and like its on fire \n",
      "4         0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
    "df = df[[0, 5]]\n",
    "df.columns = ['polarity', 'text']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc41ceb",
   "metadata": {},
   "source": [
    "### Step 3: Keep Only Positive and Negative Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5dca7",
   "metadata": {},
   "source": [
    "Here we removes neutral tweets where polarity is 2, maps the labels so 0 stays negative and 4 becomes 1 for positive.\n",
    "\n",
    "Then we print how many positive and negative tweets are left in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8321cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df[df.polarity != 2]\n",
    "\n",
    "df['polarity'] = df['polarity'].map({0: 0, 4: 1})\n",
    "\n",
    "print(df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a1f32",
   "metadata": {},
   "source": [
    "### Step 4: Clean the Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1af7b",
   "metadata": {},
   "source": [
    "Here we define a simple function to convert all text to lowercase for consistency, applies it to every tweet in the dataset.\n",
    "\n",
    "Then shows the original and cleaned versions of the first few tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85a1e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1  is upset that he can't update his Facebook by ...   \n",
      "2  @Kenichan I dived many times for the ball. Man...   \n",
      "3    my whole body feels itchy and like its on fire    \n",
      "4  @nationwideclass no, it's not behaving at all....   \n",
      "\n",
      "                                          clean_text  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - awww, t...  \n",
      "1  is upset that he can't update his facebook by ...  \n",
      "2  @kenichan i dived many times for the ball. man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(df[['text', 'clean_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090b635",
   "metadata": {},
   "source": [
    "### Step 5: Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50166605",
   "metadata": {},
   "source": [
    "This code splits the clean_text and polarity columns into training and testing sets using an 80/20 split.\n",
    "\n",
    "random_state=42 ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36343c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1280000\n",
      "Test size: 320000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean_text'],\n",
    "    df['polarity'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34de58",
   "metadata": {},
   "source": [
    "### Step 6: Perform Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f920b",
   "metadata": {},
   "source": [
    "This code creates a TF IDF vectorizer that converts text into numerical features using unigrams and bigrams limited to 5000 features.\n",
    "\n",
    "It fits and transforms the training data and transforms the test data and then prints the shapes of the resulting TF IDF matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1f83af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape (train): (1280000, 5000)\n",
      "TF-IDF shape (test): (320000, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF shape (test):\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb1818",
   "metadata": {},
   "source": [
    "### Step 7: Train Bernoulli Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045bde2",
   "metadata": {},
   "source": [
    "Here we train a Bernoulli Naive Bayes classifier on the TF IDF features from the training data.\n",
    "\n",
    "It predicts sentiments for the test data and then prints the accuracy and a detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5259e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Accuracy: 0.766478125\n",
      "\n",
      "BernoulliNB Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76    159494\n",
      "           1       0.76      0.78      0.77    160506\n",
      "\n",
      "    accuracy                           0.77    320000\n",
      "   macro avg       0.77      0.77      0.77    320000\n",
      "weighted avg       0.77      0.77      0.77    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "bnb_pred = bnb.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Bernoulli Naive Bayes Accuracy:\", accuracy_score(y_test, bnb_pred))\n",
    "print(\"\\nBernoulliNB Classification Report:\\n\", classification_report(y_test, bnb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9e134",
   "metadata": {},
   "source": [
    "### Step 9: Train Support Vector Machine (SVM) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db433c32",
   "metadata": {},
   "source": [
    "This code trains a Support Vector Machine (SVM) with a maximum of 1000 iterations on the TF IDF features.\n",
    "\n",
    "It predicts test labels then prints the accuracy and a detailed classification report showing how well the SVM performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ddcb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.79528125\n",
      "\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79    159494\n",
      "           1       0.79      0.81      0.80    160506\n",
      "\n",
      "    accuracy                           0.80    320000\n",
      "   macro avg       0.80      0.80      0.80    320000\n",
      "weighted avg       0.80      0.80      0.80    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(max_iter=1000)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "svm_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c561a",
   "metadata": {},
   "source": [
    "### Step 10: Train Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4171bde",
   "metadata": {},
   "source": [
    "This code trains a Logistic Regression model with up to 100 iterations on the TF IDF features.\n",
    "\n",
    "It predicts sentiment labels for the test data and prints the accuracy and detailed classification report for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c40681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.79539375\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79    159494\n",
      "           1       0.79      0.81      0.80    160506\n",
      "\n",
      "    accuracy                           0.80    320000\n",
      "   macro avg       0.80      0.80      0.80    320000\n",
      "weighted avg       0.80      0.80      0.80    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=100)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "logreg_pred = logreg.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logreg_pred))\n",
    "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, logreg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932e894",
   "metadata": {},
   "source": [
    "### Step 11: Make Predictions on sample Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c67b8f",
   "metadata": {},
   "source": [
    "This code takes three sample tweets and transforms them into TF IDF features using the same vectorizer.\n",
    "\n",
    "It then predicts their sentiment using the trained BernoulliNB, SVM and Logistic Regression models and prints the results for each classifier.\n",
    "\n",
    "Where 1 stands for Positive and 0 for Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a935b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Predictions:\n",
      "BernoulliNB: [1 0 1]\n",
      "SVM: [1 0 1]\n",
      "Logistic Regression: [1 0 1]\n"
     ]
    }
   ],
   "source": [
    "sample_tweets = [\"I love this!\", \"I hate that!\", \"It was okay, not great.\"]\n",
    "sample_vec = vectorizer.transform(sample_tweets)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"BernoulliNB:\", bnb.predict(sample_vec))\n",
    "print(\"SVM:\", svm.predict(sample_vec))\n",
    "print(\"Logistic Regression:\", logreg.predict(sample_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03d81f",
   "metadata": {},
   "source": [
    "We can see that our models are working fine and giving same predictions even with different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff18a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
